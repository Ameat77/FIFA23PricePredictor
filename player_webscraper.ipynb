{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats_p1(soup): #Gets basic information such as player name, club, nation, league, age, etc\n",
    "    info_table = soup.find(\"table\", {\"class\" : \"table table-info\"})\n",
    "    headers = []\n",
    "    for item in info_table.findAll(\"th\"):\n",
    "        headers.append(item.text.strip())\n",
    "    descriptions = []\n",
    "    count = 0\n",
    "    for j in info_table.findAll(\"tr\"):\n",
    "        des = j.find_all(\"td\")[0].text.strip()\n",
    "        if headers[count].lower() == \"accelerate\": #if we are looking at the accelerate\n",
    "            if des[0] == \"E\":\n",
    "                des = \"Explosive\"\n",
    "            elif des[0] == \"C\":\n",
    "                des = \"Controlled\"\n",
    "            else:\n",
    "                des = \"Lengthy\"\n",
    "        descriptions.append(des)                 \n",
    "        count += 1\n",
    "    return headers, descriptions\n",
    "\n",
    "def get_stats_p2(soup): #Gets player ratings, stats, overall, and position\n",
    "    stats = soup.find(\"div\", {\"id\": \"player_stats_json\"})\n",
    "    stats = stats.text.strip()\n",
    "    stats = json.loads(stats)[0]\n",
    "    categories = [\"Overall\", \"Position\", \"Pace\", \"Passing\", \"Shooting\", \"Dribbling\", \"Defending\", \"Physical\"]\n",
    "    values = []\n",
    "    #This block will get a player's overall and position\n",
    "    values.append((soup.find(\"div\", {\"class\": \"pcdisplay-rat\"}).text))\n",
    "    values.append((soup.find(\"div\", {\"class\": \"pcdisplay-pos\"}).text))\n",
    "    if values[1] == \"GK\":\n",
    "        return get_stats_p2_gk(soup)\n",
    "    for cat in categories:\n",
    "        if (cat.lower() != \"overall\" and cat.lower() != \"position\"):\n",
    "            values.append(stats[cat.lower()][0][\"value\"])\n",
    "    return categories, values\n",
    "\n",
    "def get_stats_p2_gk(soup): #Gets player ratings, stats, overall, and position\n",
    "    stats = soup.find(\"div\", {\"id\": \"player_stats_json\"})\n",
    "    stats = stats.text.strip()\n",
    "    stats = json.loads(stats)[0]\n",
    "    categories = [\"Overall\", \"Position\", \"Gkdiving\", \"Gkhandling\", \"Gkkicking\", \"Gkreflexes\", \"Speed\", \"Gkpositioning\"]\n",
    "    values = []\n",
    "    #This block will get a player's overall and position\n",
    "    values.append((soup.find(\"div\", {\"class\": \"pcdisplay-rat\"}).text))\n",
    "    values.append((soup.find(\"div\", {\"class\": \"pcdisplay-pos\"}).text))\n",
    "    for cat in categories:\n",
    "        if (cat.lower() != \"overall\" and cat.lower() != \"position\"):\n",
    "            values.append(stats[cat.lower()][0][\"value\"])\n",
    "    return categories, values\n",
    "\n",
    "\n",
    "def get_price_data_daily(player_id): #Gets price data on a player and adds it to a csv (for normal players)\n",
    "    response = requests.get(\n",
    "  url='https://proxy.scrapeops.io/v1/',\n",
    "  params={\n",
    "      'api_key': 'ecea4014-8c14-480c-845d-a7ff7ad269ec',\n",
    "      'url': 'https://www.futbin.com/23/playerGraph?player=' + player_id +'&year=23&type=daily_graph', \n",
    "  },\n",
    ")\n",
    "    string = response.content.decode(\"utf-8\")\n",
    "    dic = json.loads(string)\n",
    "    path = \"./Player_Data/\" + player_id\n",
    "    if (os.path.exists(path) == False):\n",
    "        os.mkdir(path)\n",
    "    if \"ps\" in dic:\n",
    "        data_path = \"./Player_Data/\" + player_id + \"/psdaily_graphFIFA23.csv\" #Path to csv data\n",
    "        lis = dic[\"ps\"]\n",
    "        time_stamp = []\n",
    "        price = []\n",
    "        for item in lis:\n",
    "            time_stamp.append(item[0])\n",
    "            price.append(item[1])\n",
    "        headers = [\"Unix Time\", \"Price\"]\n",
    "\n",
    "        with open(data_path, 'w', encoding='UTF8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            # write the header\n",
    "            writer.writerow(headers)\n",
    "            for i in range(len(price)):\n",
    "                # write the data\n",
    "                writer.writerow([time_stamp[i], price[i]])\n",
    "    if \"pc\" in dic:\n",
    "        data_path = \"./Player_Data/\" + player_id + \"/pcdaily_graphFIFA23.csv\" #Path to csv data\n",
    "        lis = dic[\"pc\"]\n",
    "        time_stamp = []\n",
    "        price = []\n",
    "        for item in lis:\n",
    "            time_stamp.append(item[0])\n",
    "            price.append(item[1])\n",
    "        headers = [\"Unix Time\", \"Price\"]\n",
    "\n",
    "        with open(data_path, 'w', encoding='UTF8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            # write the header\n",
    "            writer.writerow(headers)\n",
    "            for i in range(len(price)):\n",
    "                # write the data\n",
    "                writer.writerow([time_stamp[i], price[i]])\n",
    "    else:\n",
    "        data_path = \"./Player_Data/\" + player_id + \"/nodata.txt\"\n",
    "        with open(data_path, 'w') as fp:\n",
    "            pass\n",
    "        \n",
    "def get_price_data_yday(player_id): #Gets price data on a player and adds it to a csv (for normal players)\n",
    "    response = requests.get(\n",
    "  url='https://proxy.scrapeops.io/v1/',\n",
    "  params={\n",
    "      'api_key': 'ecea4014-8c14-480c-845d-a7ff7ad269ec',\n",
    "      'url': 'https://www.futbin.com/23/playerGraph?player=' + player_id +'&year=23&type=yesterday', \n",
    "  },\n",
    ")\n",
    "    string = response.content.decode(\"utf-8\")\n",
    "    dic = json.loads(string)\n",
    "    path = \"./Player_Data/\" + player_id\n",
    "    if (os.path.exists(path) == False):\n",
    "        os.mkdir(path)\n",
    "    if \"ps\" in dic:\n",
    "        lis = dic[\"ps\"]\n",
    "        dt = datetime.datetime.utcfromtimestamp(int(lis[0][0])/1000)\n",
    "        time = (str(dt)[0:10])\n",
    "        data_path = \"./Player_Data/\" + player_id + \"/\" + time + \"psydaygraphFIFA23.csv\" #Path to csv data\n",
    "        time_stamp = []\n",
    "        price = []\n",
    "        for item in lis:\n",
    "            time_stamp.append(item[0])\n",
    "            price.append(item[1])\n",
    "        headers = [\"Unix Time\", \"Price\"]\n",
    "        with open(data_path, 'w', encoding='UTF8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            # write the header\n",
    "            writer.writerow(headers)\n",
    "            for i in range(len(price)):\n",
    "                # write the data\n",
    "                writer.writerow([time_stamp[i], price[i]])\n",
    "    if \"pc\" in dic:\n",
    "        lis = dic[\"pc\"]\n",
    "        dt = datetime.datetime.utcfromtimestamp(int(lis[0][0])/1000)\n",
    "        time = (str(dt)[0:10])\n",
    "        data_path = \"./Player_Data/\" + player_id + \"/\" + time + \"pcyesterday_graphFIFA23.csv\" #Path to csv data\n",
    "        time_stamp = []\n",
    "        price = []\n",
    "        for item in lis:\n",
    "            time_stamp.append(item[0])\n",
    "            price.append(item[1])\n",
    "        headers = [\"Unix Time\", \"Price\"]\n",
    "        with open(data_path, 'w', encoding='UTF8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            # write the header\n",
    "            writer.writerow(headers)\n",
    "            for i in range(len(price)):\n",
    "                # write the data\n",
    "                writer.writerow([time_stamp[i], price[i]])\n",
    "    else:\n",
    "        data_path = \"./Player_Data/\" + player_id + \"/nodata.txt\"\n",
    "        with open(data_path, 'w') as fp:\n",
    "            pass\n",
    "        \n",
    "                 \n",
    "\n",
    "\n",
    "def get_player_dic(player_link):#Creates a player_dic\n",
    "    r = requests.get(\n",
    "      url='https://proxy.scrapeops.io/v1/',\n",
    "      params={\n",
    "          'api_key': 'ecea4014-8c14-480c-845d-a7ff7ad269ec',\n",
    "          'url': player_link, \n",
    "      },\n",
    "    )\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    results = str(soup.prettify())\n",
    "    actual_id = results[results.find(\"data-player-resource\") + 22: results.find(\"data-position\") - 2]\n",
    "    headers = []\n",
    "    descriptions = []\n",
    "    h1, d1 = get_stats_p1(soup)\n",
    "    h2, d2 = get_stats_p2(soup)\n",
    "    headers = h1 + h2\n",
    "    descriptions = d1 + d2\n",
    "    dic = {}\n",
    "    irrelevant = [\"Origin\", \"Added on\", \"Club ID\", \"League ID\", \"B.Type\", \"R.Face\"]\n",
    "    for i in range(len(headers)):\n",
    "        if headers[i] == \"Height\":\n",
    "            dic[headers[i]] = descriptions[i][:3]\n",
    "        elif headers[i] == \"Age\":\n",
    "            dic[headers[i]] = descriptions[i][:2]\n",
    "        elif headers[i] not in irrelevant:\n",
    "            dic[headers[i]] = descriptions[i]\n",
    "    dic[\"ID\"] = actual_id\n",
    "    player_id = dic[\"ID\"]\n",
    "    path = \"./Player_Data/\" + player_id\n",
    "    if (os.path.exists(path) == False):\n",
    "        os.mkdir(path)\n",
    "    with open(\"./Player_Data/\" + player_id + \"/infoFIFA23.txt\", 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(dic))\n",
    "    return dic\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "player_links = []\n",
    "#Format of Link to loop through player_dataset: https://www.futbin.com/players?page=1\n",
    "for i in range(100):\n",
    "    r = requests.get(\n",
    "          url='https://proxy.scrapeops.io/v1/',\n",
    "          params={\n",
    "              'api_key': 'ecea4014-8c14-480c-845d-a7ff7ad269ec',\n",
    "              'url': \"https://www.futbin.com/players?page=\" + str(i+1), \n",
    "          },\n",
    "        )\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    player_lis = (soup.findAll(\"tr\", {\"class\": \"player_tr_2\"}))\n",
    "    for item in player_lis:\n",
    "        item = str(item)\n",
    "        string = \"https://www.futbin.com\" + item[item.find(\"data\") + 10: item.find(\">\") - 1]\n",
    "        player_links.append(string)\n",
    "    player_lis = (soup.findAll(\"tr\", {\"class\": \"player_tr_1\"}))\n",
    "    for item in player_lis:\n",
    "        item = str(item)\n",
    "        string = \"https://www.futbin.com\" + item[item.find(\"data\")  + 10: item.find(\">\") - 1]\n",
    "        player_links.append(string)\n",
    "    if (i % 5 == 0):\n",
    "        print(player_links[-10:])\n",
    "with open(\"playerlinks.txt\", 'w') as convert_file:\n",
    "    convert_file.write(json.dumps(player_links))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "#We are on 18 for i\n",
    "for i in range(106, len(player_links)):\n",
    "    link = player_links[i]\n",
    "    dic = get_player_dic(link)\n",
    "    player_id = dic[\"ID\"]\n",
    "    player_dic[player_id] = dic\n",
    "    get_price_data_daily(player_id)\n",
    "    get_price_data_yday(player_id)\n",
    "    print(i)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.futbin.com/23/player/174/lev-yashin'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_links[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block of code gets the player's price data\n",
    "#Format of Code to get Prices\n",
    "#Hourly Graph (today): https://www.futbin.com/23/playerGraph?type=today&year=23&player=173731\n",
    "#Hourly Graph (yday): https://www.futbin.com/23/playerGraph?type=yesterday&year=23&player=173731\n",
    "#Daily Graph: https://www.futbin.com/23/playerGraph?player=173731&year=23&type=daily_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        writer.writerow([time_stamp[i], price[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path, sep=\",\")\n",
    "# shows top 10 rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So now we have price data, what's the next plan:\n",
    "'''\n",
    "When in day should I buy a player\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gkdiving': [{'shortcut': 'DIV', 'gk_shortcut': 'DIV', 'chem_change': 1, 'id': 'gkdiving', 'stat_num': 'igs4', 'name': 'Diving', 'gk_stat_name': 'Diving', 'type': 'main', 'value': 95, 'max': True}, {'shortcut': 'DIV', 'chem_change': 1, 'id': 'gkdiving', 'stat_num': 'igs5', 'name': 'Diving', 'type': 'sub', 'value': 95, 'max': True}], 'gkhandling': [{'shortcut': 'HAN', 'gk_shortcut': 'HAN', 'chem_change': 1, 'id': 'gkhandling', 'stat_num': 'igs11', 'name': 'Handling', 'gk_stat_name': 'Handling', 'type': 'main', 'value': 89, 'max': True}, {'shortcut': 'HAN', 'chem_change': 1, 'id': 'gkhandling', 'stat_num': 'igs12', 'name': 'Handling', 'type': 'sub', 'value': 89, 'max': True}], 'gkkicking': [{'shortcut': 'KIC', 'gk_shortcut': 'KIC', 'chem_change': 1, 'id': 'gkkicking', 'stat_num': 'igs18', 'name': 'Kicking', 'gk_stat_name': 'Kicking', 'type': 'main', 'value': 75, 'max': True}, {'shortcut': 'KIC', 'chem_change': 1, 'id': 'gkkicking', 'stat_num': 'igs19', 'name': 'Kicking', 'type': 'sub', 'value': 75, 'max': True}], 'gkreflexes': [{'shortcut': 'REF', 'gk_shortcut': 'REF', 'chem_change': 1, 'id': 'gkreflexes', 'stat_num': 'igs30', 'name': 'Reflexes', 'gk_stat_name': 'Reflexes', 'type': 'main', 'value': 96, 'max': True}, {'shortcut': 'REF', 'chem_change': 1, 'id': 'gkreflexes', 'stat_num': 'igs31', 'name': 'Reflexes', 'type': 'sub', 'value': 96, 'max': True}], 'speed': [{'shortcut': 'SPE', 'gk_shortcut': 'SPE', 'chem_change': 1, 'id': 'speed', 'stat_num': 'igs1', 'name': 'Speed', 'gk_stat_name': 'Speed', 'type': 'main', 'value': 60, 'max': True}, {'shortcut': 'ACC', 'chem_change': 1, 'id': 'acceleration', 'stat_num': 'igs2', 'name': 'Acceleration', 'type': 'sub', 'value': 65, 'max': True}, {'shortcut': 'S/S', 'chem_change': 1, 'id': 'sprintspeed', 'stat_num': 'igs3', 'name': 'Sprint Speed', 'type': 'sub', 'value': 53, 'max': True}], 'gkpositioning': [{'shortcut': 'POS', 'gk_shortcut': 'POS', 'chem_change': 1, 'id': 'gkpositioning', 'stat_num': 'igs24', 'name': 'Positioning', 'gk_stat_name': 'Positioning', 'type': 'main', 'value': 95, 'max': True}, {'shortcut': 'POS', 'chem_change': 1, 'id': 'gkpositioning', 'stat_num': 'igs25', 'name': 'Positioning', 'type': 'sub', 'value': 95, 'max': True}]}\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(\n",
    "      url='https://proxy.scrapeops.io/v1/',\n",
    "      params={\n",
    "          'api_key': 'ecea4014-8c14-480c-845d-a7ff7ad269ec',\n",
    "          'url': \"https://www.futbin.com/23/player/174/lev-yashin\", \n",
    "      },\n",
    "    )\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "stats = soup.find(\"div\", {\"id\": \"player_stats_json\"})\n",
    "stats = stats.text.strip()\n",
    "stats = json.loads(stats)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95, 89, 75, 96, 60, 95, '94', 'GK']\n"
     ]
    }
   ],
   "source": [
    "stats = soup.find(\"div\", {\"id\": \"player_stats_json\"})\n",
    "stats = stats.text.strip()\n",
    "stats = json.loads(stats)[0]\n",
    "categories = [\"Gkdiving\", \"Gkhandling\", \"Gkkicking\", \"Gkreflexes\", \"Speed\", \"Gkpositioning\"]\n",
    "values = []\n",
    "for cat in categories:\n",
    "    values.append(stats[cat.lower()][0][\"value\"])\n",
    "\n",
    "categories.append(\"Overall\")\n",
    "categories.append(\"Position\")\n",
    "#This block will get a player's overall and position\n",
    "values.append((soup.find(\"div\", {\"class\": \"pcdisplay-rat\"}).text))\n",
    "values.append((soup.find(\"div\", {\"class\": \"pcdisplay-pos\"}).text))\n",
    "return categories, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Name', 'AcceleRATE', 'Club', 'Nation', 'League', 'Skills', 'Weak Foot', 'Intl. Rep', 'Foot', 'Height', 'Weight', 'Revision', 'Att. WR', 'Def. WR', 'Added on', 'Origin', 'R.Face', 'B.Type', 'DOB', 'ID', 'Club ID', 'League ID'] ['Lev Yashin', 'Controlled', 'FUT ICONS', 'Russia', 'Icons', '1', '3', '4', 'Right', '189cm | 6\\'2\"', '82', 'Icon', 'Med', 'Med', '2021-08-31', 'Prime', '', 'High & Average', '22-10-1929', '238380', '112658', '2118']\n",
      "{'Name': 'Lev Yashin', 'AcceleRATE': 'Controlled', 'Club': 'FUT ICONS', 'Nation': 'Russia', 'League': 'Icons', 'Skills': '1', 'Weak Foot': '3', 'Intl. Rep': '4', 'Foot': 'Right', 'Height': '189', 'Weight': '82', 'Revision': 'Icon', 'Att. WR': 'Med', 'Def. WR': 'Med', 'DOB': '22-10-1929', 'ID': '238380', 'Gkdiving': '94', 'Gkhandling': 'GK', 'Gkkicking': 95, 'Gkreflexes': 89, 'Speed': 75, 'Gkpositioning': 96, 'Overall': 60, 'Position': 95}\n"
     ]
    }
   ],
   "source": [
    "print(get_player_dic(\"https://www.futbin.com/23/player/174/lev-yashin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
